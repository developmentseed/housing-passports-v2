{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training\n",
    "\n",
    "For classification, we've implemented the tiny variant of the `ConvNeXtV2` architecture with a few tweaks.\n",
    "\n",
    "### ConvNexT V2 Tiny\n",
    "\n",
    "[ConvNexT V2](https://github.com/facebookresearch/ConvNeXt-V2) Tiny is a fully convolutional neural network architecture designed for efficient image classification tasks. Here are the key points to understand about ConvNexT V2 Tiny:\n",
    "\n",
    "1. **Architecture**: ConvNexT V2 Tiny is a variant of the ConvNexT architecture, which is known for its efficient and effective design. It consists of a series of convolutional layers organized in a hierarchical structure with cross-stage feature aggregation (CSFA) modules combined with a masked auto-encoder. The masked auto-encoder enables the benefits of self-supervised learning.\n",
    "\n",
    "2. **Efficiency**: ConvNexT V2 Tiny is designed to be lightweight and computationally efficient, making it suitable for deployment on resource-constrained devices such as mobile phones or edge devices.\n",
    "\n",
    "3. **Drop Path Regularization**: The architecture includes drop path regularization, which randomly drops connections between layers during training to prevent overfitting and improve generalization performance.\n",
    "\n",
    "4. **Pretraining**: ConvNexT V2 Tiny models are often pretrained on large-scale image datasets such as ImageNet to learn generic features before being fine-tuned on specific tasks.\n",
    "\n",
    "5. **Usage**: ConvNexT V2 Tiny models can be used for various computer vision tasks, including image classification, object detection, and semantic segmentation. They provide a balance between model complexity and performance, making them suitable for real-world applications where computational resources are limited.\n",
    "\n",
    "Overall, ConvNexT V2 Tiny offers a lightweight and efficient solution for image classification tasks, making it a popular choice for applications where model size and computational efficiency are critical considerations.\n",
    "\n",
    "\n",
    "We've implemented this architecture for training a classifier model to predict building properties using [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/). It leverages the [AIM (AI Model Tracking)](https://aimstack.io/) platform for experiment logging and monitoring.\n",
    "\n",
    "## Main classifier components\n",
    "\n",
    "The `classifier_train.py` script defines a PyTorch Lightning module called `HPClassifier`, which serves as a neural network model for a multi-class classification task. \n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - The `HPClassifier` class inherits from `LightningModule` and defines the neural network architecture.\n",
    "   - The backbone of the model is based on the \"convnextv2_tiny\" architecture from the [Timm library](https://github.com/huggingface/pytorch-image-models), which is pretrained and includes drop path regularization.\n",
    "   - Separate classifier heads are defined for each building property (e.g., completeness, condition, material, security, use) using the `NormMlpClassifierHead` layer from Timm.\n",
    "   - Each classifier head predicts the corresponding property using the backbone features.\n",
    "\n",
    "3. **Metrics**:\n",
    "   - F1 Score: F1 score metrics are computed for each property using the `torchmetrics.F1Score` class.\n",
    "\n",
    "4. **Optimizer and Scheduler**:\n",
    "   - AdamW optimizer with a configurable learning rate (`lr`) is used.\n",
    "   - A MultiStepLR scheduler is employed to adjust the learning rate at predefined epochs.\n",
    "\n",
    "5. **Loss Functions**:\n",
    "   - Cross-entropy loss is used for each property prediction with class weights to handle class imbalance (see more on this below).\n",
    "   - Focal loss is commented out but can be used as an alternative loss function.\n",
    "\n",
    "6. **Training, Validation, and Testing Steps**:\n",
    "   - `training_step`, `validation_step`, and `test_step` methods define the forward pass and loss computation for training, validation, and testing phases, respectively.\n",
    "   - Losses and metrics are logged for monitoring the training progress using PyTorch Lightning logging utilities.\n",
    "\n",
    "7. **Hyperparameters**:\n",
    "   - Hyperparameters such as learning rate (`lr`) and the number of classes for each building property are configurable during model initialization.\n",
    "\n",
    "## Class imbalance\n",
    "The following entails the relative percentages of class-wise representation in the dataset.\n",
    "```\n",
    "complete\n",
    "complete      83.713137\n",
    "incomplete    16.286863\n",
    "\n",
    "condition\n",
    "fair    62.441354\n",
    "poor    29.733579\n",
    "good     7.825067\n",
    "\n",
    "material\n",
    "plaster                                      63.505362\n",
    "mix-other-unclear                            23.927614\n",
    "brick_or_cement-concrete_block                5.981903\n",
    "wood_polished                                 4.205764\n",
    "corrugated_metal                              1.105898\n",
    "stone_with_mud-ashlar_with_lime_or_cement     0.561327\n",
    "wood_crude-plank                              0.435657\n",
    "container-trailer                             0.276475\n",
    "\n",
    "security\n",
    "unsecured    74.857574\n",
    "secured      25.142426\n",
    "\n",
    "use\n",
    "residential                85.547922\n",
    "commercial                  6.719169\n",
    "mixed                       6.710791\n",
    "critical_infrastructure     1.022118\n",
    "```\n",
    "We weight the losses per class as following:\n",
    "\n",
    "```\n",
    "complete\n",
    "complete = 0.16\n",
    "incomplete = 0.84\n",
    "\n",
    "condition\n",
    "poor = 0.19\n",
    "fair = 0.09\n",
    "good = 0.72\n",
    "\n",
    "material\n",
    "mix-other-unclear = 0.0046\n",
    "plaster = 0.0017\n",
    "brick_or_cement-concrete_block = 0.018\n",
    "wood_polished = 0.026\n",
    "stone_with_mud-ashlar_with_lime_or_cement = 0.196\n",
    "corrugated_metal = 0.099\n",
    "wood_crude-plank = 0.25\n",
    "container-trailer = 0.39\n",
    "\n",
    "security\n",
    "secured = 0.75\n",
    "unsecured = 0.25\n",
    "\n",
    "use\n",
    "residential = 0.009\n",
    "critical_infrastructure  = 0.759\n",
    "mixed = 0.115\n",
    "commercial = 0.115\n",
    "```\n",
    "In doing so, we can force the model to learn from errors tied to the rare classes more than the popular classes.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Before training, run `prep_classifier_training_data.py` for both the left and right side annotation JSON files.\n",
    "\n",
    "1. **Command-Line Arguments**: Specify the following command-line arguments:\n",
    "   - `IMG_DIR`: Directory containing the dataset images.\n",
    "   - `ANN_JSON`: The left or right side annotation JSON file.\n",
    "   - `SIDE`: The side, \"left\" or \"right\"\n",
    "   - `OUT_DIR`: Directory to write the \"ground-truth-box-clipped-and-buffered\" images to, along with the csv file containing annotations.\n",
    "\n",
    "   Execute the script using the command:\n",
    "   `python classifier_train.py <IMG_DIR> <ANN_JSON> <SIDE> <OUT_DIR>`\n",
    "\n",
    "2. **Merge the left and right side csv files and add weights column**: \n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df_right = pd.read_csv(\"/home/ubuntu/data/cumulative_annos_right.csv\")\n",
    "df_left = pd.read_csv(\"/home/ubuntu/data/cumulative_annos_left.csv\")\n",
    "\n",
    "df_combined = pd.concat([df_right, df_left], ignore_index=True)\n",
    "df_combined[\"file_name_original\"] = df_combined[\"file_name\"]\n",
    "df_combined[\"file_name\"] = df_combined[\"image_name_clip\"]\n",
    "\n",
    "prefix_to_add = '/' # Whatever the out_dir was\n",
    "df_combined['file_name'] = prefix_to_add + df_combined['file_name']\n",
    "df_combined[\"weights\"] = 1\n",
    "df_combined.to_csv(\"data/raw/data_fixed.csv\")\n",
    "```\n",
    "\n",
    "### Now for training\n",
    "\n",
    "The script can be executed directly, taking command-line arguments for specifying experiment name, image directory, and data directory. It is designed to support a modular and configurable implementation of a neural network model for multi-class building property classification tasks, utilizing pre-trained backbone architectures and PyTorch Lightning for training and evaluation.\n",
    "\n",
    "#### Script Workflow\n",
    "1. Set seed for reproducibility using Lightning.\n",
    "2. Initialize an AIM logger for experiment tracking.\n",
    "3. Call our `HouseDataModule` object for data loading and preprocessing.\n",
    "4. Instantiate the `HPClassifier` model.\n",
    "5. Define callback functions:\n",
    "    - `LearningRateMonitor`: Monitors and logs the learning rate during training.\n",
    "    - `ModelCheckpoint`: Saves the best model based on validation performance.\n",
    "    - `BackboneFreezeUnfreeze`: Unfreezes the model backbone after a certain epoch.\n",
    "6. Initialize the Lightning Trainer with required configurations:\n",
    "    - `devices`: Automatic device selection.\n",
    "    - `accelerator`: Automatic accelerator selection.\n",
    "    - `max_epochs`: Maximum number of training epochs.\n",
    "    - `precision`: Mixed-precision training for improved efficiency.\n",
    "    - `logger`: AIM logger for experiment tracking.\n",
    "    - `callbacks`: List of callback functions.\n",
    "7. Train the model using the `fit` method with training and validation data loaders.\n",
    "    - We unfreeze the backbone architecture after 10 epochs.\n",
    "8. Test the trained model using the `test` method with test data loaders.\n",
    "\n",
    "#### How to run training\n",
    "\n",
    "To run the script, follow these steps:\n",
    "\n",
    "1. **Command-Line Arguments**: Specify the following command-line arguments:\n",
    "   - `EXPERIMENT_NAME`: Name of the training experiment.\n",
    "   - `IMG_DIR`: Directory containing the dataset images.\n",
    "   - `DATA_DIR`: Directory containing partitioned CSV files for the dataset.\n",
    "\n",
    "2. **Run Script**: Execute the script using the command:\n",
    "\n",
    "`python classifier_train.py <EXPERIMENT_NAME> <IMG_DIR> <DATA_DIR>`\n",
    "\n",
    "Replace `<EXPERIMENT_NAME>`, `<IMG_DIR>`, and `<DATA_DIR>` with the appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual display of training data\n",
    "\n",
    "These are images clipped and buffered by the ground truth bounding box. The property labels are indicated in the caption.\n",
    "\n",
    "\n",
    "![alt text](images/groundtruth_classifier_0.png)\n",
    "![alt text](images/groundtruth_classifier_1.png)\n",
    "![alt text](images/groundtruth_classifier_2.png)\n",
    "![alt text](images/groundtruth_classifier_3.png)\n",
    "![alt text](images/groundtruth_classifier_4.png)\n",
    "![alt text](images/groundtruth_classifier_5.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
