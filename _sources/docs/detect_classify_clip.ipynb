{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "With trained detection and classification models, we can now combine the two in stepwise manner to predict classified building facades. \n",
    "\n",
    "The main workflow here entails:\n",
    "1) Detect building facades in streetview images\n",
    "2) Clip the images by detected building (using again, a buffer of 100 pixels to be consistent with what the classifier expects)\n",
    "3) Classify building properties from the clipped image that focuses on the detected building\n",
    "\n",
    "From this, we can attribute building facades with the properties that we trained for: completeness, condition, material, use and security.\n",
    "\n",
    "### How to run\n",
    "\n",
    "Please ensure you have your streetview images on hand along with the trained model weights.\n",
    "\n",
    "The final detection model weights are located at `s3://hp-deliverables-v2/detectron2_model/50000iter/model_final.pth`.\n",
    "\n",
    "The final classification model weights are located at `s3://hp-deliverables-v2/classifier_models/512/final/63869a95ae444b3d87f78331/checkpoints/epoch:48-step:12838-loss:2.419-f1:4.245.ckpt`.\n",
    "\n",
    "To run the script `detect_clip_classify.py`, execute it from the command line with the following arguments:\n",
    "\n",
    "1. **IMG_DIR**: Directory containing dataset images.\n",
    "2. **DET_CPKT_PATH**: Path to the object detection model checkpoint file.\n",
    "3. **CLASS_CPKT_PATH**: Path to the classification model checkpoint file.\n",
    "4. **OUTPUT_DIR**: Directory to save output files (clipped images and predicitions csv (`detection_classification_predictions.csv`)).\n",
    "\n",
    "Usage: `python detect_clip_classify.py <IMG_DIR> <DET_CPKT_PATH> <CLASS_CPKT_PATH> <OUTPUT_DIR>`\n",
    "\n",
    "Example command:\n",
    "\n",
    "```bash\n",
    "python detect_clip_classify.py images/ detector_model.pth classification_model.pth output/\n",
    "```\n",
    "\n",
    "The `detection_classification_predictions.csv` file contains fields for the original streetview image name, the predicted bounding box, the predicted bounding box score, each predicted building property, as well as the name of the clipped image name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual results\n",
    "\n",
    "#### Un-clipped, with boxes and classes:\n",
    "\n",
    "![alt text](images/detcl_2EBrjKYZlWkm7nOFpMA5i8_right_567139785572031_right.png)\n",
    "![alt text](images/detcl_2mRY68ZzMIfGEcoiBLulv1_right_858022839291090_right.png)\n",
    "![alt text](images/detcl_D92FXsNMnj6pxVCTbiGrJk_left_363186852769069_left.png)\n",
    "![alt text](images/detcl_5CTauPtRpsQqwSczy1jXNK_right_1046263043047172_right_1.png)\n",
    "![alt text](images/detcl_f8xmBnLl4eKb735GH1Wzhu_right_253390323756976_right.png)\n",
    "![alt text](images/detcl_DEV9xX4Kaiy87PBGf3YNpg_left_218525031263357_left.png)\n",
    "![alt text](images/det_cl_TRXnCb1kLNUKeo89Bpt5wv_left_1357243005281547_left.png)\n",
    "![alt text](images/detcl_UbzfhJAWL2N5vO3HXGEZtp_left_295844279444850_left.png)\n",
    "\n",
    "\n",
    "#### Clipped, with classes\n",
    "\n",
    "![alt text](images/detect_classify_pred0_tTRGjavPoSelZNYc1O2qbu_left_254330320664391_left.png)\n",
    "![alt text](images/detect_classify_pred11_4r8Be9MPU03VNa16QWfTGO_right_881083906861807_right.png)\n",
    "![alt text](images/detect_classify_pred3_Pg5loJ2tx1B8ZYzEU4NpfC_left_1012573259800054_left.png)\n",
    "![alt text](images/detect_classify_pred8_IVRwdhS1g08YiHm4t23JTA_left_265829086071996_left.png)\n",
    "![alt text](images/detect_classify_pred4_0mQlNMAy4x6C7hZJcGXWOe_left_332634499403525_left.png)\n",
    "![alt text](images/detect_classify_pred6_NvChPFk9mfMrldz0WRgGbH_right_804625321456050_right.png)\n",
    "![alt text](images/detect_classify_pred13_2EBrjKYZlWkm7nOFpMA5i8_left_835715341581970_left.png)\n",
    "![alt text](images/detect_classify_pred12_Pg5loJ2tx1B8ZYzEU4NpfC_right_1356927844943950_right_1.png)\n",
    "![alt text](images/detect_classify_pred2_2EBrjKYZlWkm7nOFpMA5i8_left_251740414247834_left_1.png)\n",
    "![alt text](images/detect_classify_pred10_tTRGjavPoSelZNYc1O2qbu_right_1246075890128886_right.png)\n",
    "![alt text](images/detect_classify_pred7_KUeyNt9kjrsMzlJ8X2dmf0_left_2226435360894990_left.png)\n",
    "![alt text](images/detect_classify_pred14_4r8Be9MPU03VNa16QWfTGO_left_3190611904573392_left.png)\n",
    "![alt text](images/detect_classify_pred5_wg47LWeMDXvsERoYrQB8hG_right_1072653617082899_right.png)\n",
    "![alt text](images/detect_classify_pred1_toOC03Pxcn51gwvyGVHrMW_left_369043715529607_left_1.png)\n",
    "![alt text](images/detect_classify_pred14_pHTQL7F3a2WCZJnwx4di0y_left_259487610432232_left.png)\n",
    "![alt text](images/detect_classify_pred15_8qTcoVK6MZFdCJXLGgnk35_right_1230875951637787_right.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
