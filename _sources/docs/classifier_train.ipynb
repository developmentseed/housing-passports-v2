{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Training\n",
    "\n",
    "For classification, we've implemented the tiny variant of the `ConvNeXtV2` architecture with a few tweaks.\n",
    "\n",
    "### ConvNexT V2 Tiny\n",
    "\n",
    "[ConvNexT V2](https://github.com/facebookresearch/ConvNeXt-V2) Tiny is a fully convolutional neural network architecture designed for efficient image classification tasks. Here are the key points to understand about ConvNexT V2 Tiny:\n",
    "\n",
    "1. **Architecture**: ConvNexT V2 Tiny is a variant of the ConvNexT architecture, which is known for its efficient and effective design. It consists of a series of convolutional layers organized in a hierarchical structure with cross-stage feature aggregation (CSFA) modules combined with a masked auto-encoder. The masked auto-encoder enables the benefits of self-supervised learning.\n",
    "\n",
    "2. **Efficiency**: ConvNexT V2 Tiny is designed to be lightweight and computationally efficient, making it suitable for deployment on resource-constrained devices such as mobile phones or edge devices.\n",
    "\n",
    "3. **Drop Path Regularization**: The architecture includes drop path regularization, which randomly drops connections between layers during training to prevent overfitting and improve generalization performance.\n",
    "\n",
    "4. **Pretraining**: ConvNexT V2 Tiny models are often pretrained on large-scale image datasets such as ImageNet to learn generic features before being fine-tuned on specific tasks.\n",
    "\n",
    "5. **Usage**: ConvNexT V2 Tiny models can be used for various computer vision tasks, including image classification, object detection, and semantic segmentation. They provide a balance between model complexity and performance, making them suitable for real-world applications where computational resources are limited.\n",
    "\n",
    "Overall, ConvNexT V2 Tiny offers a lightweight and efficient solution for image classification tasks, making it a popular choice for applications where model size and computational efficiency are critical considerations.\n",
    "\n",
    "\n",
    "We've implemented this architecture for training a classifier model to predict building properties using [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/). It leverages the [AIM (AI Model Tracking)](https://aimstack.io/) platform for experiment logging and monitoring.\n",
    "\n",
    "## Main classifier components\n",
    "\n",
    "The `classifier_train.py` script defines a PyTorch Lightning module called `HPClassifier`, which serves as a neural network model for a multi-class classification task. \n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - The `HPClassifier` class inherits from `LightningModule` and defines the neural network architecture.\n",
    "   - The backbone of the model is based on the \"convnextv2_tiny\" architecture from the [Timm library](https://github.com/huggingface/pytorch-image-models), which is pretrained and includes drop path regularization.\n",
    "   - Separate classifier heads are defined for each building property (e.g., completeness, condition, material, security, use) using the `NormMlpClassifierHead` layer from Timm.\n",
    "   - Each classifier head predicts the corresponding property using the backbone features.\n",
    "\n",
    "3. **Metrics**:\n",
    "   - F1 Score: F1 score metrics are computed for each property using the `torchmetrics.F1Score` class.\n",
    "\n",
    "4. **Optimizer and Scheduler**:\n",
    "   - AdamW optimizer with a configurable learning rate (`lr`) is used. The base learning rate is set to 0.001.\n",
    "   - A MultiStepLR scheduler is employed to adjust the learning rate at predefined epochs.\n",
    "\n",
    "5. **Loss Functions**:\n",
    "   - Cross-entropy loss is used for each property prediction with class weights to handle class imbalance (see more on this below).\n",
    "   - Focal loss is commented out but can be used as an alternative loss function.\n",
    "\n",
    "6. **Training, Validation, and Testing Steps**:\n",
    "   - `training_step`, `validation_step`, and `test_step` methods define the forward pass and loss computation for training, validation, and testing phases, respectively.\n",
    "   - Losses and metrics are logged for monitoring the training progress using PyTorch Lightning logging utilities.\n",
    "\n",
    "7. **Hyperparameters**:\n",
    "   - Hyperparameters such as learning rate (`lr`) and the number of classes for each building property are configurable during model initialization. A maximum of 50 epochs was allowed for.\n",
    "\n",
    "## Class imbalance\n",
    "The following entails the relative percentages of class-wise representation in the dataset.\n",
    "```\n",
    "complete\n",
    "complete      83.713137\n",
    "incomplete    16.286863\n",
    "\n",
    "condition\n",
    "fair    62.441354\n",
    "poor    29.733579\n",
    "good     7.825067\n",
    "\n",
    "material\n",
    "plaster                                      63.505362\n",
    "mix-other-unclear                            23.927614\n",
    "brick_or_cement-concrete_block                5.981903\n",
    "wood_polished                                 4.205764\n",
    "corrugated_metal                              1.105898\n",
    "stone_with_mud-ashlar_with_lime_or_cement     0.561327\n",
    "wood_crude-plank                              0.435657\n",
    "container-trailer                             0.276475\n",
    "\n",
    "security\n",
    "unsecured    74.857574\n",
    "secured      25.142426\n",
    "\n",
    "use\n",
    "residential                85.547922\n",
    "commercial                  6.719169\n",
    "mixed                       6.710791\n",
    "critical_infrastructure     1.022118\n",
    "```\n",
    "We weight the losses per class as following:\n",
    "\n",
    "```\n",
    "complete\n",
    "complete = 0.16\n",
    "incomplete = 0.84\n",
    "\n",
    "condition\n",
    "poor = 0.19\n",
    "fair = 0.09\n",
    "good = 0.72\n",
    "\n",
    "material\n",
    "mix-other-unclear = 0.0046\n",
    "plaster = 0.0017\n",
    "brick_or_cement-concrete_block = 0.018\n",
    "wood_polished = 0.026\n",
    "stone_with_mud-ashlar_with_lime_or_cement = 0.196\n",
    "corrugated_metal = 0.099\n",
    "wood_crude-plank = 0.25\n",
    "container-trailer = 0.39\n",
    "\n",
    "security\n",
    "secured = 0.75\n",
    "unsecured = 0.25\n",
    "\n",
    "use\n",
    "residential = 0.009\n",
    "critical_infrastructure  = 0.759\n",
    "mixed = 0.115\n",
    "commercial = 0.115\n",
    "```\n",
    "In doing so, we can force the model to learn from errors tied to the rare classes more than the popular classes.\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Environment set-up\n",
    "To train the models in this project, a `g5.2xlarge` (single [NVIDIA A10G Tensor Core GPU](https://aws.amazon.com/ec2/instance-types/g5/)) machine based on the `Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) 20240116` AMI was set up. See more on this AMI at: https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html. It is recommended to set up a machine with a similar runtime.\n",
    "\n",
    "If you haven't done so already (for the detection training), from your local machine or virtual machine equipped with a GPU, run the following (requires a [mamba](https://mamba.readthedocs.io/en/latest/index.html) installation):\n",
    "```\n",
    "mamba env create --file environment.yml\n",
    "python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "```\n",
    "\n",
    "You should now have an environment called `hp` that has everything installed. Make sure you have a GPU available with versions compatible with the following:\n",
    "```\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2023 NVIDIA Corporation\n",
    "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
    "Cuda compilation tools, release 12.1, V12.1.105\n",
    "Build cuda_12.1.r12.1/compiler.32688072_0\n",
    "torch:  2.1 ; cuda:  cu121\n",
    "```\n",
    "### Prepare the data\n",
    "Before training, run `prep_classifier_training_data.py` for both the left and right side annotation JSON files. This script take the annotations, clips the streetview images by the bounding boxes buffered with 100 pixels on each side (as dimensions and box placement permit) and writes the classes and filenames to a CSV file that will be used in the next step of the classification preparation. We tested a few different buffer sizes for clipping, and found that 100 pixels provides the best balance of focus to the subject building and beneficial surrounding context. Also, if you're wondering why we are clipping the images by the boxes before classification, it is because some streetview images have more than one building and therefore more than one annotation associated with them.\n",
    "\n",
    "You can find the data we've already processed here: `s3://hp-deliverables-v2/classifier_data/final/`\n",
    " \n",
    "1. **Command-Line Arguments**: Specify the following command-line arguments:\n",
    "   - `IMG_DIR`: Directory containing the dataset images.\n",
    "   - `ANN_JSON`: The left or right side annotation JSON file.\n",
    "   - `SIDE`: The side, \"left\" or \"right\"\n",
    "   - `OUT_DIR`: Directory to write the \"ground-truth-box-clipped-and-buffered\" images to, along with the csv file containing annotations.\n",
    "\n",
    "   Execute the script using the command:\n",
    "   `python classifier_train.py <IMG_DIR> <ANN_JSON> <SIDE> <OUT_DIR>`\n",
    "\n",
    "2. **Merge the left and right side csv files and add weights column**: \n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df_right = pd.read_csv(\"/home/ubuntu/data/cumulative_annos_right.csv\")\n",
    "df_left = pd.read_csv(\"/home/ubuntu/data/cumulative_annos_left.csv\")\n",
    "\n",
    "df_combined = pd.concat([df_right, df_left], ignore_index=True)\n",
    "df_combined[\"file_name_original\"] = df_combined[\"file_name\"]\n",
    "df_combined[\"file_name\"] = df_combined[\"image_name_clip\"]\n",
    "\n",
    "prefix_to_add = \"\" # Whatever the out_dir was e.g. \"clipped_images/\"\n",
    "df_combined['file_name'] = prefix_to_add + df_combined['file_name']\n",
    "df_combined[\"weights\"] = 1\n",
    "df_combined.to_csv(\"data/raw/data_fixed.csv\")\n",
    "```\n",
    "\n",
    "### Now for training\n",
    "\n",
    "The script can be executed directly, taking command-line arguments for specifying experiment name, image directory, and data directory. It is designed to support a modular and configurable implementation of a neural network model for multi-class building property classification tasks, utilizing pre-trained backbone architectures and PyTorch Lightning for training and evaluation.\n",
    "\n",
    "#### Script Workflow\n",
    "1. Set seed for reproducibility using Lightning.\n",
    "2. Initialize an AIM logger for experiment tracking.\n",
    "3. Call our `HouseDataModule` object for data loading and preprocessing.\n",
    "4. Instantiate the `HPClassifier` model.\n",
    "5. Define callback functions:\n",
    "    - `LearningRateMonitor`: Monitors and logs the learning rate during training.\n",
    "    - `ModelCheckpoint`: Saves the best model based on validation performance.\n",
    "    - `BackboneFreezeUnfreeze`: Unfreezes the model backbone after a certain epoch.\n",
    "6. Initialize the Lightning Trainer with required configurations:\n",
    "    - `devices`: Automatic device selection.\n",
    "    - `accelerator`: Automatic accelerator selection.\n",
    "    - `max_epochs`: Maximum number of training epochs.\n",
    "    - `precision`: Mixed-precision training for improved efficiency.\n",
    "    - `logger`: AIM logger for experiment tracking.\n",
    "    - `callbacks`: List of callback functions.\n",
    "7. Train the model using the `fit` method with training and validation data loaders.\n",
    "    - We unfreeze the backbone architecture after 10 epochs.\n",
    "\n",
    "Here are the final model's loss curve and F1 score curve:\n",
    "![alt text](images/lossclass.png)\n",
    "![alt text](images/f1class.png)\n",
    "\n",
    "#### How to run training\n",
    "\n",
    "To run the script, follow these steps:\n",
    "\n",
    "1. **Command-Line Arguments**: Specify the following command-line arguments:\n",
    "   - `EXPERIMENT_NAME`: Name of the training experiment.\n",
    "   - `IMG_DIR`: Directory containing the dataset images.\n",
    "   - `DATA_DIR`: Directory containing partitioned CSV files for the dataset.\n",
    "\n",
    "2. **Run Script**: Execute the script using the command:\n",
    "\n",
    "`python classifier_train.py <EXPERIMENT_NAME> <IMG_DIR> <DATA_DIR>`\n",
    "\n",
    "Replace `<EXPERIMENT_NAME>`, `<IMG_DIR>`, and `<DATA_DIR>` with the appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual display of training data\n",
    "\n",
    "These are images clipped and buffered by the ground truth bounding box. The property labels are indicated in the caption.\n",
    "\n",
    "\n",
    "![alt text](images/groundtruth_classifier_0.png)\n",
    "![alt text](images/groundtruth_classifier_1.png)\n",
    "![alt text](images/groundtruth_classifier_2.png)\n",
    "![alt text](images/groundtruth_classifier_3.png)\n",
    "![alt text](images/groundtruth_classifier_4.png)\n",
    "![alt text](images/groundtruth_classifier_5.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
